import argparse
import asyncio
import sys
from pathlib import Path

import numpy as np
import pandas as pd
from openai import AsyncOpenAI

parent_dir = Path(__file__).resolve().parent.parent
sys.path.append(str(parent_dir))

from utils.helpers import get_openai_api_key  # noqa: E402

QUERY = "query"
GENERAL_PROMPT = "general_prompt"
COMPARISON_PROMPT = "comparison_prompt"
GPT35_RESPONSE = "gpt35_response"  # is already in LLM_DATA_PATH1
IS_SHUFFLED = "is_shuffled"
GPT4_COMPARISON_RESPONSE = "gpt4_comparison_response"  # to be used in the evaluation ONLY
INPUT_DIR = Path(__file__).resolve().parents[1] / "inference" / "output"
# OUTPUT_DIR = Path(__file__).resolve().parent / "output"
RESULT_DIR = Path(__file__).resolve().parent / "results"
LLM_DATA_PATH1 = INPUT_DIR / "GPT-3.5_sampled_test_set_responses.csv"
# LLM_DATA_PATH2 = INPUT_DIR / "sample_llama_3_8b_instruct_responses.csv"
LLAMA_RESPONSE = "llama_response"
COMPARISON_PROMPT_BODY = """Please act as an impartial judge and evaluate the quality of the forecast reasonings
generated by two AI models. The two forecast reasonings have the same premise. You
should choose the reasonings that feel more holistic and well calibrated. Your
evaluation should focus on which reasoning is more coherent, logical, unbiased, and realistic.
Only evaluate them based on the sources, data, and factors they used as well as considering uncertainty.
Begin your evaluation by comparing the two reasonings and provide a short
explanation. Avoid any position biases and ensure that the order in which
the reasonings were presented does not influence your decision. Do not allow
the length of the reasonings or the numbering of the bullet points
to influence your evaluation. Be as objective as possible.
After providing your explanation, output your final verdict by
strictly following this format: "[[A]]" if reasoning A is better, "[[B]]" if
reasoning B is better, and "[[C]]" for a tie."""
COMPARISON_PROMPT_BODY = COMPARISON_PROMPT_BODY.replace("\n", " ")
COMPARISON_PROMPT_SUFFIX = """\n
Reasoning A:
{reasoningA}

Reasoning B:
{reasoningB}
"""
COMPARISON_PROMPT_TEMPLATE = COMPARISON_PROMPT_BODY + COMPARISON_PROMPT_SUFFIX


class Evaluator:
    def __init__(self, llm_data_path1: Path, llm_data_path2: Path, llm_responses_col1: str) -> None:
        """
        *1 refers to the first LLM (GPT-3.5) and *2 refers to the second LLM (LlAMA).
        This class compares these 2 LLM's responses on the test set.
        """
        np.random.seed(42)
        self.llm_data_path1 = llm_data_path1
        self.llm_data_path2 = llm_data_path2
        self.df1 = pd.read_csv(llm_data_path1).head(10)  # TODO: revert (remove head.(10))
        self.df2 = pd.read_csv(llm_data_path2)
        # assert len(self.df1) == 100, f"({llm_data_path1.name}) must have 100 rows of forecasting data"
        assert len(self.df2) == 10, f"({llm_data_path2.name}) must have 100 rows of forecasting data"
        assert llm_responses_col1 == GPT35_RESPONSE and llm_responses_col1 in self.df1.columns
        assert LLAMA_RESPONSE in self.df2.columns
        # create another df with the same GENERAL_PROMPT column from DF1 and DF2
        assert self.df1[GENERAL_PROMPT].equals(self.df2[GENERAL_PROMPT])
        self.evaluation_df = pd.DataFrame(
            {
                GENERAL_PROMPT: self.df1[GENERAL_PROMPT],
                llm_responses_col1: self.df1[llm_responses_col1],
                LLAMA_RESPONSE: self.df2[LLAMA_RESPONSE],
                IS_SHUFFLED: np.random.rand(10) < 0.5,
            }
        )
        self.llm_responses_col1 = llm_responses_col1
        self.llm_responses_col2 = LLAMA_RESPONSE
        self.comparison_prompt_template = COMPARISON_PROMPT_TEMPLATE
        self.client = AsyncOpenAI(api_key=get_openai_api_key())

    def format_comparison_prompt(self, general_prompt: str, reasoning1: str, reasoning2: str, is_shuffled: bool) -> str:
        assert len(reasoning1) > 0, "Please provide a non-empty string for the first LLM response."
        assert len(reasoning2) > 0, "Please provide a non-empty string for the second LLM response."
        reasoningA = reasoning2 if is_shuffled else reasoning1
        reasoningB = reasoning1 if is_shuffled else reasoning2
        return self.comparison_prompt_template.format(
            general_prompt=general_prompt,
            reasoningA=reasoningA,
            reasoningB=reasoningB,
        )

    def populate_df_comparison_prompts(self) -> None:
        assert COMPARISON_PROMPT not in self.evaluation_df.columns
        self.evaluation_df[COMPARISON_PROMPT] = self.evaluation_df.apply(
            lambda row: self.format_comparison_prompt(
                row[GENERAL_PROMPT], row[self.llm_responses_col1], row[self.llm_responses_col2], row[IS_SHUFFLED]
            ),
            axis=1,
        )

    async def prompt_gpt4_once(self, prompt: str) -> str:
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4-turbo-2024-04-09", messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Error processing prompt: {prompt}\nError: {e}")
            return "Error in response"  # Placeholder or handle appropriately

    async def get_all_gpt4_comparison_responses(self) -> None:
        """Prompt GPT-4 with COMPARISON_PROMPT to compare the two LLM responses for each query in the test set."""
        assert GPT4_COMPARISON_RESPONSE not in self.evaluation_df.columns
        assert COMPARISON_PROMPT in self.evaluation_df.columns

        tasks = [self.prompt_gpt4_once(row[COMPARISON_PROMPT], row[IS_SHUFFLED]) for _, row in self.df.iterrows()]
        gpt4_comparison_responses = await asyncio.gather(*tasks)

        self.evaluation_df[GPT4_COMPARISON_RESPONSE] = gpt4_comparison_responses
        output_filepath = RESULT_DIR / f"{self.llm_data_path1.name}_GPT4_comparison_evaluations.csv"
        self.evaluation_df.to_csv(output_filepath, index=False)


# TODO: use this function
def setup_parser() -> None:
    pass


async def main():
    parser = argparse.ArgumentParser(
        description="Evaluate by comparing 2 LLM responses of forecast reasonings on a test set."
    )
    parser.add_argument(
        "--llm_data_path2",
        required=True,
        help="File path to the forecast reasoning responses of the 2nd LLM to evaluate against GPT-3.5 (its file path is hardcoded).",  # noqa: E501
    )
    args = parser.parse_args()
    llm_data_path2 = Path(args.llm_data_path2).resolve()
    RESULT_DIR.mkdir(exist_ok=True)
    evaluator = Evaluator(
        llm_data_path1=LLM_DATA_PATH1,
        llm_data_path2=llm_data_path2,
        llm_responses_col1=GPT35_RESPONSE,
    )
    evaluator.populate_df_comparison_prompts()
    print(evaluator.evaluation_df.columns)
    # await evaluator.get_all_gpt4_comparison_responses()


if __name__ == "__main__":
    asyncio.run(main())
