import argparse
import asyncio
import sys
from pathlib import Path

import pandas as pd
from openai import AsyncOpenAI

parent_dir = Path(__file__).resolve().parent.parent
sys.path.append(str(parent_dir))

from utils.helpers import get_openai_api_key  # noqa: E402

QUERY = "query"
GENERAL_PROMPT = "general_prompt"
COMPARISON_PROMPT = "comparison_prompt"
GPT35_RESPONSE = "gpt35_response"  # is already in LLM_DATA_PATH1
GPT4_COMPARISON_RESPONSE = "gpt4_comparison_response"  # to be used in the evaluation ONLY
INPUT_DIR = Path(__file__).resolve().parents[1] / "inference" / "output"
OUTPUT_DIR = Path(__file__).resolve().parent / "output"
LLM_DATA_PATH1 = INPUT_DIR / "GPT-3.5_sampled_test_set_responses.csv"
LLM_DATA_PATH2 = INPUT_DIR / "sample_llama_3_8b_instruct_responses.csv"
LLAMA_3_8B_RESPONSE = "llama_3_8b_instruct_response"  # TODO: change this
COMPARISON_PROMPT_BODY = """Please act as an impartial judge and evaluate the quality of the forecast reasonings
generated by two AI models. The two forecast reasonings have the same premise. You
should choose the reasonings that feel more holistic and well calibrated. Your
evaluation should focus on which reasoning is more coherent, logical, unbiased, and realistic.
Only evaluate them based on the sources, data, and factors they used as well as considering uncertainty.
Begin your evaluation by comparing the two reasonings and provide a short
explanation. Avoid any position biases and ensure that the order in which
the reasonings were presented does not influence your decision. Do not allow
the length of the reasonings to influence your evaluation. Be as objective as
possible. After providing your explanation, output your final verdict by
strictly following this format: "[[A]]" if reasoning A is better, "[[B]]" if
reasoning B is better, and "[[C]]" for a tie."""
COMPARISON_PROMPT_BODY = COMPARISON_PROMPT_BODY.replace("\n", " ")
COMPARISON_PROMPT_SUFFIX = """\n
Reasoning A:
{reasoning1}

Reasoning B:
{reasoning2}
"""
COMPARISON_PROMPT_TEMPLATE = COMPARISON_PROMPT_BODY + COMPARISON_PROMPT_SUFFIX


class Evaluator:
    def __init__(
        self, llm_data_path1: Path, llm_data_path2: Path, llm_responses_col1: str, llm_responses_col2: str
    ) -> None:
        """
        *1 refers to the first LLM (GPT-3.5) and *2 refers to the second LLM (LlAMA).
        This class compares these 2 LLM's responses on the test set.
        """
        self.llm_data_path1 = llm_data_path1
        self.llm_data_path2 = llm_data_path2
        self.df1 = pd.read_csv(llm_data_path1)
        self.df2 = pd.read_csv(llm_data_path2)
        assert len(self.df1) == 100, f"({llm_data_path1.name}) must have 100 rows of forecasting data"
        assert len(self.df2) == 100, f"({llm_data_path2.name}) must have 100 rows of forecasting data"
        assert llm_responses_col1 == GPT35_RESPONSE and llm_responses_col1 in self.df1.columns
        assert llm_responses_col2 in self.df2.columns
        # create another df with the same GENERAL_PROMPT column from DF1 and DF2
        assert self.df1[GENERAL_PROMPT].equals(self.df2[GENERAL_PROMPT])
        # combined df (evaluation_df) doesn't need question or resolution criteria columns
        self.evaluation_df = pd.DataFrame(
            {
                GENERAL_PROMPT: self.df1[GENERAL_PROMPT],
                llm_responses_col1: self.df1[llm_responses_col1],
                llm_responses_col2: self.df2[llm_responses_col2],
            }
        )
        self.llm_responses_col1 = llm_responses_col1
        self.llm_responses_col2 = llm_responses_col2
        self.comparison_prompt_template = COMPARISON_PROMPT_TEMPLATE
        self.client = AsyncOpenAI(api_key=get_openai_api_key())

    def format_comparison_prompt(self, general_prompt: str, reasoning1: str, reasoning2: str) -> str:
        assert len(reasoning1) > 0, "Please provide a non-empty string for the first LLM response."
        assert len(reasoning2) > 0, "Please provide a non-empty string for the second LLM response."
        return self.comparison_prompt_template.format(
            general_prompt=general_prompt,
            reasoning1=reasoning1,
            reasoning2=reasoning2,
        )

    def populate_df_comparison_prompts(self) -> None:
        assert COMPARISON_PROMPT not in self.evaluation_df.columns
        self.evaluation_df[COMPARISON_PROMPT] = self.evaluation_df.apply(
            lambda row: self.format_comparison_prompt(
                row[GENERAL_PROMPT], row[self.llm_responses_col1], row[self.llm_responses_col2]
            ),
            axis=1,
        )

    # TODO: ask GPT-4 to rank
    async def prompt_gpt4_once(self, prompt: str) -> str:
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4-turbo-2024-04-09", messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Error processing prompt: {prompt}\nError: {e}")
            return "Error in response"  # Placeholder or handle appropriately

    async def get_all_gpt4_comparison_responses(self) -> None:
        """Prompt GPT-4 with COMPARISON_PROMPT to compare the two LLM responses for each query in the test set."""
        assert GPT4_COMPARISON_RESPONSE not in self.evaluation_df.columns
        assert COMPARISON_PROMPT in self.evaluation_df.columns

        raise NotImplementedError("TODO: shuffle the two reasonings")
        tasks = [self.prompt_gpt35_once(row[COMPARISON_PROMPT]) for _, row in self.df.iterrows()]
        gpt4_comparison_responses = await asyncio.gather(*tasks)

        self.evaluation_df[GPT4_COMPARISON_RESPONSE] = gpt4_comparison_responses
        output_filepath = OUTPUT_DIR / "GPT4_comparison_evaluations.csv"
        self.df.to_csv(output_filepath, index=False)


# TODO: use this function
def setup_parser() -> None:
    pass


async def main():
    parser = argparse.ArgumentParser(
        description="Evaluate by comparing 2 LLM responses of forecast reasonings on a test set."
    )
    parser.add_argument(
        "--llm_data_path2",
        default=LLM_DATA_PATH2,
        help="File path to the forecast reasoning responses of the 2nd LLM to evaluate against GPT-3.5 (its file path is hardcoded).",  # noqa: E501
    )
    parser.add_argument(
        "--llm_responses_col2",
        default=LLAMA_3_8B_RESPONSE,
        help="Column in LLM_DATA_PATH2 that contains the LLM responses. Not programmatically found right now to avoid ambiguity.",  # noqa: E501
    )
    args = parser.parse_args()
    llm_data_path2 = args.llm_data_path2
    llm_responses_col2 = args.llm_responses_col2
    OUTPUT_DIR.mkdir(exist_ok=True)
    evaluator = Evaluator(
        llm_data_path1=LLM_DATA_PATH1,
        llm_data_path2=llm_data_path2,
        llm_responses_col1=GPT35_RESPONSE,
        llm_responses_col2=llm_responses_col2,
    )
    evaluator.populate_df_comparison_prompts()
    # await evaluator.get_all_gpt4_comparison_responses()


if __name__ == "__main__":
    asyncio.run(main())
